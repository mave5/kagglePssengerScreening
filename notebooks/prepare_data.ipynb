{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition: Passenger Screening Algorithm Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data preparation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "#from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.animation as anim\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import glob\n",
    "from os.path import basename\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# get package versions\n",
    "def get_version(*vars):\n",
    "    for var in vars:\n",
    "        module = __import__(var)    \n",
    "        print ('%s: %s' %(var,module.__version__))\n",
    "    \n",
    "# package version    \n",
    "get_version('numpy','matplotlib','cv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COLORMAP = 'pink'\n",
    "\n",
    "# data folder\n",
    "path2data='../data/'\n",
    "\n",
    "# select data extension\n",
    "extension='*.aps'\n",
    "\n",
    "# path to raw data\n",
    "# raw .aps files are located here\n",
    "path2rawData=path2data\n",
    "\n",
    "# stage 1 labels\n",
    "path2_stage1_labels=path2data+\"/stage1_labels.csv\"\n",
    "\n",
    "# sample submission\n",
    "path2_stage1_samplesubmission=path2data+\"/stage1_sample_submission.csv\"\n",
    "path2_stage2_samplesubmission=path2data+\"stage2_sample_submission.csv\"\n",
    "\n",
    "# outputs are stored here\n",
    "path2stage1_train=path2data+'stage1_train.hdf5'\n",
    "path2stage1_test=path2data+'stage1_test.hdf5'\n",
    "path2stage2_test=path2data+'stage2_test.hdf5'\n",
    "\n",
    "# body zone photo\n",
    "path2bodyzone=path2data+'body_zones.png'\n",
    "\n",
    "# body zone descriptions\n",
    "# zone orders in the csv file are like this\n",
    "zones=['zone1','zone10','zone11','zone12',\n",
    "       'zone13','zone14','zone15','zone16',\n",
    "       'zone17','zone2','zone3','zone4',\n",
    "       'zone5','zone6','zone7','zone8','zone9']\n",
    "\n",
    "body_zone_desc={\n",
    "        'zone1' :'Right Bicep',\n",
    "        'zone10':'Upper left Hip/thigh',\n",
    "        'zone11':'Lower Right Thigh',\n",
    "        'zone12':'Lower left Thigh',\n",
    "        'zone13':'Right Calf',\n",
    "        'zone14':'Left Calf(below knee)',\n",
    "        'zone15':'Right Ankle Bone',\n",
    "        'zone16':'Left Ankle Bone',\n",
    "        'zone17':'Upper Back',    \n",
    "        'zone2':'Right Forearm',\n",
    "        'zone3':'Left Bicep',\n",
    "        'zone4':'Left Forearm',\n",
    "        'zone5':'Upper Chest',\n",
    "        'zone6':'Right Rib Cage and Abs',\n",
    "        'zone7':'Left Side Rib Cage and Abs',\n",
    "        'zone8':'Upper Right Hip/Tigh',\n",
    "        'zone9':'Groin (Sensetive area)'\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the threat zones\n",
    "body_zones_img = plt.imread(path2bodyzone)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.imshow(body_zones_img);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e69d0f57-8a32-4bfd-b505-2a29e6404f7d",
    "_execution_state": "idle",
    "_uuid": "87d8411dc21ac9edf0fc0f0dbc0f037fd3c9f34a",
    "collapsed": false
   },
   "source": [
    "### Read header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b669e9d4-389e-471b-a26e-7b9f59724af4",
    "_uuid": "d2bbd58d7b33d594459aa6bbb8fc4147668a1ee1",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_header(infile):\n",
    "    \"\"\"Read image header (first 512 bytes)\n",
    "    \"\"\"\n",
    "    h = dict()\n",
    "    fid = open(infile, 'r+b')\n",
    "    h['filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "    h['parent_filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "    h['comments1'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "    h['comments2'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "    h['energy_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['config_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['file_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['trans_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scan_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['data_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['date_modified'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 16))\n",
    "    h['frequency'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['mat_velocity'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['num_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_polarization_channels'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['spare00'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['adc_min_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['adc_max_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['band_width'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['spare01'] = np.fromfile(fid, dtype = np.int16, count = 5)\n",
    "    h['polarization_type'] = np.fromfile(fid, dtype = np.int16, count = 4)\n",
    "    h['record_header_size'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['word_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['word_precision'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['min_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['max_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['avg_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['data_scale_factor'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['data_units'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['surf_removal'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['edge_weighting'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['x_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['y_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['z_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['t_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "    h['spare02'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['x_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['scan_orientation'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scan_direction'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['data_storage_order'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scanner_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['x_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['t_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['num_x_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_y_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_z_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['num_t_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "    h['x_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['date_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "    h['time_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "    h['depth_recon'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['elevation_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['roll_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['azimuth_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['adc_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['spare06'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "    h['scanner_radius'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['x_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['y_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['z_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['t_delay'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['range_gate_start'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['range_gate_end'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['ahis_software_version'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "    h['spare_end'] = np.fromfile(fid, dtype = np.float32, count = 10)\n",
    "    return h\n",
    "\n",
    "# unit test ----------------------------------\n",
    "#APS_FILE_NAME = path+'/00360f79fd6e02781457eda48f85da90.aps'\n",
    "#header = read_header(APS_FILE_NAME)\n",
    "\n",
    "#for data_item in sorted(header):\n",
    "    #print ('{} -> {}'.format(data_item, header[data_item]))\n",
    "    \n",
    "# display a sample subject\n",
    "def dispSampleSubject(X,y_zone): \n",
    "    # X shape: N*C*H*W\n",
    "    sbj_num=np.random.randint(len(X))\n",
    "    print ('subject: %s' %sbj_num)\n",
    "    #array_stats(X[sbj_num])\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for k in range(16):\n",
    "        plt.subplot(4,4,k+1)\n",
    "        plt.imshow(X[sbj_num,k],cmap='gray')\n",
    "\n",
    "    # zones with objects\n",
    "    nz_label=np.nonzero(y_zone[sbj_num,:])[0]\n",
    "    for nz_l in nz_label:\n",
    "        print ('%s: %s' %(zones[nz_l],body_zone_desc[zones[nz_l]]))\n",
    "    return sbj_num    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "32371ee0-c472-4247-8ab7-bb5b12c164ce",
    "_execution_state": "idle",
    "_uuid": "81bc02741351c9f640d41ab18b60663721e82246",
    "collapsed": false
   },
   "source": [
    "### Read image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(infile):\n",
    "    \"\"\"Read any of the 4 types of image files, returns a numpy array of the image contents\n",
    "    \"\"\"\n",
    "    extension = os.path.splitext(infile)[1]\n",
    "    h = read_header(infile)\n",
    "    nx = int(h['num_x_pts'])\n",
    "    ny = int(h['num_y_pts'])\n",
    "    nt = int(h['num_t_pts'])\n",
    "    fid = open(infile, 'rb')\n",
    "    fid.seek(512) #skip header\n",
    "    if extension == '.aps' or extension == '.a3daps':\n",
    "        if(h['word_type']==7): #float32\n",
    "            data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n",
    "        elif(h['word_type']==4): #uint16\n",
    "            data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n",
    "        data = data * h['data_scale_factor'] #scaling factor\n",
    "        data = data.reshape(nx, ny, nt, order='F').copy() #make N-d image\n",
    "    elif extension == '.a3d':\n",
    "        if(h['word_type']==7): #float32\n",
    "            data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n",
    "        elif(h['word_type']==4): #uint16\n",
    "            data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n",
    "        data = data * h['data_scale_factor'] #scaling factor\n",
    "        data = data.reshape(nx, nt, ny, order='F').copy() #make N-d image\n",
    "    elif extension == '.ahi':\n",
    "        data = np.fromfile(fid, dtype = np.float32, count = 2* nx * ny * nt)\n",
    "        data = data.reshape(2, ny, nx, nt, order='F').copy()\n",
    "        real = data[0,:,:,:].copy()\n",
    "        imag = data[1,:,:,:].copy()\n",
    "    fid.close()\n",
    "    if extension != '.ahi':\n",
    "        return data\n",
    "    else:\n",
    "        return real, imag\n",
    "    \n",
    "    \n",
    "\n",
    "matplotlib.rc('animation', html='html5')\n",
    "\n",
    "def plot_image(path):\n",
    "    data = read_data(path)\n",
    "    \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    def animate(i):\n",
    "        im = ax.imshow(np.flipud(data[:,:,i].transpose()), cmap = 'viridis')\n",
    "        return [im]\n",
    "    return anim.FuncAnimation(fig, animate, frames=range(0,data.shape[2]), interval=200, blit=True)\n",
    "\n",
    "def dump_to_video(dbpath, video_path):\n",
    "    data = read_data(dbpath)\n",
    "    w, h, n = data.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 2.0, (w, h)) \n",
    "\n",
    "    for i in range(n):\n",
    "        img = np.flipud(data[:,:,i].transpose())\n",
    "        norm = plt.Normalize()\n",
    "        img = norm(img) \n",
    "        img = plt.cm.viridis(img)\n",
    "        img = (255.0 * img).astype(np.uint8)\n",
    "        out.write(img)\n",
    "\n",
    "    out.release()\n",
    "    \n",
    "def convert_to_grayscale(img):\n",
    "    # scale pixel values to grayscale\n",
    "    base_range = np.amax(img) - np.amin(img)\n",
    "    rescaled_range = 255 - 0\n",
    "    img_rescaled = (((img - np.amin(img)) * rescaled_range) / base_range)\n",
    "\n",
    "    return np.uint8(img_rescaled)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load list of subjects and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stagenn='stage1_'\n",
    "\n",
    "# get list of files\n",
    "path2raw1=path2rawData+stagenn+extension[2:]+'/'+extension\n",
    "print('looking into '+path2raw1) \n",
    "list_of_files=glob.glob(path2raw1)\n",
    "print ('number of files: %s' %(len(list_of_files)))\n",
    "\n",
    "# read labels\n",
    "stage1_train=pd.read_csv(path2_stage1_labels)\n",
    "stage1_test=pd.read_csv(path2_stage1_samplesubmission)\n",
    "\n",
    "\n",
    "# number of zones\n",
    "nb_zones=17\n",
    "print ('stage 1 train labels:', len(stage1_train))\n",
    "print ('stage 1 train subjects:', len(stage1_train)/nb_zones)\n",
    "print ('stage 1 test labels:', len(stage1_test))\n",
    "print ('stage 1 test subjects: ', len(stage1_test)/nb_zones)\n",
    "\n",
    "# labels\n",
    "Probability=stage1_train.Probability\n",
    "y=np.array(Probability)\n",
    "\n",
    "n1=int(len(y)/nb_zones)\n",
    "y=np.reshape(y,(n1,nb_zones))\n",
    "print ('labels shape: '+str( y.shape))\n",
    "\n",
    "\n",
    "# stats of labels\n",
    "#nb_y_perclass=np.count_nonzero(y,axis=0)\n",
    "#print ('labels per zone: %s' %nb_y_perclass)\n",
    "#print ('labels per zone pcn: %s' %(1.*nb_y_perclass/n1*100.))\n",
    "#plt.figure(figsize=(15,5))\n",
    "#plt.stem((1.*nb_y_perclass/n1*100.))\n",
    "#plt.xticks(range(nb_zones), zones)\n",
    "#plt.title('# labels per zone')\n",
    "#plt.xlabel('zones')\n",
    "#plt.ylabel('# labels %')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with data: fetch and dispaly sample video/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path2image=np.random.choice(list_of_files)\n",
    "\n",
    "# get file name\n",
    "file_name=basename(path2image)\n",
    "print ('sample video: '+ file_name)\n",
    "\n",
    "# dump as video\n",
    "path2video='./output/videos/'+file_name+'.avi'\n",
    "#dump_to_video(path2image,path2video)\n",
    "\n",
    "#rnd_num=np.random.randint(len(list_of_files))\n",
    "#path2image=list_of_files[rnd_num]\n",
    "X= read_data(path2image)\n",
    "Xcp=X.transpose()\n",
    "print ('video id, shape: ', file_name, X.shape)\n",
    "\n",
    "# get label\n",
    "sbj_num=stage1_train.index[stage1_train.Id==file_name[:-4]+'_Zone1']/nb_zones\n",
    "print (sbj_num)\n",
    "print (y.shape)\n",
    "print (y[sbj_num,:])\n",
    "\n",
    "\n",
    "# zones with objects\n",
    "nz_label=np.nonzero(y[sbj_num,:])[1]\n",
    "for nz_l in nz_label:\n",
    "    print ('%s: %s' %(zones[nz_l],body_zone_desc[zones[nz_l]]))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for k in range(16):\n",
    "    plt.subplot(4,4,k+1)\n",
    "    #plt.imshow(X[:,:,k],cmap='gray')\n",
    "    Xk=np.flipud(Xcp[k])\n",
    "    plt.imshow(Xk,cmap=COLORMAP)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### playing with data: thresholding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false \n",
    "# this part is not needed\n",
    "img = convert_to_grayscale(Xk)\n",
    "# global thresholding\n",
    "ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in xrange(3):\n",
    "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
    "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
    "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading stage1 train data and store as HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix='stage1_train'\n",
    "stagenn='stage1_'\n",
    "\n",
    "# location to hdf5 data\n",
    "path2hdf5=path2data+prefix+'.hdf5'\n",
    "print('checking local file: %s' %path2hdf5)\n",
    "if not os.path.exists(path2hdf5):\n",
    "    sg1_h5=h5py.File(path2hdf5,'w-')\n",
    "    for k,row_num in enumerate(range(0,len(stage1_train),nb_zones)):\n",
    "        id=stage1_train.Id[row_num][:-6]\n",
    "        print ('subject id: %s %s' %(k, id))\n",
    "        path2image=path2rawData+stagenn+extension[2:]+'/'+id+extension[1:]\n",
    "        X= read_data(path2image)\n",
    "        print (X.shape,X.dtype,np.min(X),np.max(X))\n",
    "        sg1_h5[id]=X\n",
    "    sg1_h5['labels']=y\n",
    "    sg1_h5.close()\n",
    "else:\n",
    "    ### verify hf5\n",
    "    print(path2hdf5+' exist!')\n",
    "    sg1_h5=h5py.File(path2hdf5,'r')\n",
    "    print ('number of files in %s: %s' %(prefix,len(sg1_h5.keys())))\n",
    "    print ('labels:', sg1_h5['labels'].shape)\n",
    "    id0=sg1_h5.keys()[0]\n",
    "    print (sg1_h5[id0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### playing with data: visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys=sg1_h5.keys()\n",
    "\n",
    "### sample image and label\n",
    "sbj_num=np.random.randint(len(keys))\n",
    "\n",
    "id=keys[sbj_num]\n",
    "print ('subject id: '+ id)\n",
    "\n",
    "path2image=path2rawData+stagenn+extension[2:]+'/'+id+extension[1:]\n",
    "\n",
    "X= read_data(path2image)\n",
    "#Xcp=X.transpose()\n",
    "print ('subject, shape: ',sbj_num, X.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for k in range(16):\n",
    "    plt.subplot(4,4,k+1)\n",
    "    #Xk=np.flipud(Xcp[k])\n",
    "    plt.imshow(X[:,:,k],cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading stage1-test data and save as hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix='stage1_test'\n",
    "stagenn=\"stage1_\"\n",
    "\n",
    "# location of hdf5\n",
    "path2hdf5=path2data+prefix+'.hdf5'\n",
    "print('checking: '+path2hdf5)\n",
    "if not os.path.exists(path2hdf5):\n",
    "    sg1_h5=h5py.File(path2hdf5,'w-')\n",
    "    for k,row_num in enumerate(range(0,len(stage1_test),nb_zones)):\n",
    "        id=stage1_test.Id[row_num][:-6]\n",
    "        print ('subject id: %s %s' %(k, id))\n",
    "        path2image=path2rawData+stagenn+extension[2:]+'/'+id+extension[1:]\n",
    "        X= read_data(path2image)\n",
    "        print (X.shape,X.dtype,np.min(X),np.max(X))\n",
    "        sg1_h5[id]=X\n",
    "    #sg1_h5['labels']=y # there is no label \n",
    "    sg1_h5.close()\n",
    "else:\n",
    "    ### verify hf5\n",
    "    print(path2hdf5+' exist!')\n",
    "    sg1_h5=h5py.File(path2hdf5,'r')\n",
    "    print ('number of files in %s: %s' %(prefix,len(sg1_h5.keys())))\n",
    "    id0=sg1_h5.keys()[0]\n",
    "    print (sg1_h5[id0].shape)\n",
    "    \n",
    "    try:\n",
    "        print ('labels:', sg1_h5['labels'].shape)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys=sg1_h5.keys()\n",
    "\n",
    "### sample image and label\n",
    "sbj_num=np.random.randint(len(keys))\n",
    "\n",
    "id=keys[sbj_num]\n",
    "print ('subject id: '+ id)\n",
    "\n",
    "path2image=path2rawData+stagenn+extension[2:]+'/'+id+extension[1:]\n",
    "\n",
    "X= read_data(path2image)\n",
    "#Xcp=X.transpose()\n",
    "print ('subject, shape: ',sbj_num, X.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "#Xr=cv2.resize(X,(w,h),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "for k in range(16):\n",
    "    plt.subplot(4,4,k+1)\n",
    "    #Xk=np.flipud(Xcp[k])\n",
    "    plt.imshow(X[:,:,k],cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stage 2 sample submission\n",
    "stage2_test=pd.read_csv(path2_stage2_samplesubmission)\n",
    "stage2_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix='stage2_test'\n",
    "stagenn='stage2_'\n",
    "\n",
    "# location of hdf5\n",
    "path2hdf5=path2data+prefix+'.hdf5'\n",
    "print(path2hdf5)\n",
    "\n",
    "if not os.path.exists(path2hdf5):\n",
    "    sg1_h5=h5py.File(path2hdf5,'w-')\n",
    "    for k,row_num in enumerate(range(0,len(stage2_test),nb_zones)):\n",
    "        id=stage2_test.Id[row_num][:-6]\n",
    "        print ('subject id: %s %s' %(k, id))\n",
    "        path2image=path2rawData+stagenn+extension[2:]+'/'+id+extension[1:]\n",
    "        X= read_data(path2image)\n",
    "        print (X.shape,X.dtype,np.min(X),np.max(X))\n",
    "        sg1_h5[id]=X\n",
    "    #sg1_h5['labels']=y # there is no label \n",
    "    sg1_h5.close()\n",
    "else:\n",
    "    ### verify hf5\n",
    "    print(path2hdf5+' exist!')\n",
    "    sg2_h5=h5py.File(path2hdf5,'r')\n",
    "    print ('number of files in %s: %s' %(prefix,len(sg2_h5.keys())))\n",
    "    id0=sg2_h5.keys()[0]\n",
    "    print (sg2_h5[id0].shape)\n",
    "    \n",
    "    try:\n",
    "        print ('labels:', sg1_h5['labels'].shape)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys=sg2_h5.keys()\n",
    "\n",
    "### sample image and label\n",
    "sbj_num=np.random.randint(len(keys))\n",
    "\n",
    "id=keys[sbj_num]\n",
    "print ('subject id: '+ id)\n",
    "\n",
    "path2image=path2rawData+stagenn+extension[2:]+'/'+id+extension[1:]\n",
    "\n",
    "X= read_data(path2image)\n",
    "#Xcp=X.transpose()\n",
    "print ('subject, shape: ',sbj_num, X.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "#Xr=cv2.resize(X,(w,h),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "for k in range(16):\n",
    "    plt.subplot(4,4,k+1)\n",
    "    #Xk=np.flipud(Xcp[k])\n",
    "    plt.imshow(X[:,:,k],cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  downsample/stack data into one array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(data_type='stage1_train'):\n",
    "    # number of zones\n",
    "    nb_zones=17\n",
    "\n",
    "    if data_type=='stage1_train':\n",
    "        ff=h5py.File(path2stage1_train,'r')\n",
    "        # get subject ids\n",
    "        keys=ff.keys()\n",
    "        ids=np.array(keys[:-1])\n",
    "        y=ff['labels'].value\n",
    "    elif data_type=='stage1_leader':\n",
    "        ff=h5py.File(path2stage1_test,'r') \n",
    "        keys=ff.keys()\n",
    "        ids=np.array(keys)\n",
    "        y=np.zeros((nb_zones,len(ids)),'uint8')\n",
    "    elif data_type=='stage2_leader':\n",
    "        ff=h5py.File(path2stage2_test,'r') \n",
    "        keys=ff.keys()\n",
    "        ids=np.array(keys)\n",
    "        y=np.zeros((nb_zones,len(ids)),'uint8')\n",
    "        \n",
    "    X=[]\n",
    "    for k,id1 in enumerate(ids):\n",
    "        print (k,id1)\n",
    "        X0= ff[id1].value\n",
    "        X0=cv2.resize(X0,(w,h),interpolation = cv2.INTER_CUBIC)\n",
    "        X.append(X0)\n",
    "    X=np.stack(X)\n",
    "    X=np.transpose(X,(0,3,1,2))\n",
    "    return X,y.astype('uint8'),ids\n",
    "\n",
    "# this was to rotate images\n",
    "def rotateData(data_type,(h,w),rotationAngle=180):\n",
    "    path2traintest=path2data+data_type+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "    path2traintestR=path2data+data_type+str(rotationAngle)+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "\n",
    "    # load train-test data\n",
    "    if os.path.exists(path2traintest):\n",
    "        print('loading %s' %path2traintest)\n",
    "        ff_traintest=h5py.File(path2traintest,'r')\n",
    "        X=ff_traintest['X']\n",
    "        try:\n",
    "            y=ff_traintest['y'].value\n",
    "        except:\n",
    "            y=np.zeros(len(X))\n",
    "        ids=ff_traintest['ids'].value\n",
    "    else:\n",
    "        IOError('source data does not exist!')\n",
    "        \n",
    "    \n",
    "    if not os.path.exists(path2traintestR):\n",
    "        print('storing %s' %path2traintestR)\n",
    "        print('wait ...')\n",
    "        ## rotate\n",
    "        n,c,h,w=X.shape\n",
    "        if (rotationAngle // 90) % 2 ==0:\n",
    "            Xr=np.zeros((n,c,h,w),dtype=X.dtype)\n",
    "        else:\n",
    "            Xr=np.zeros((n,c,w,h),dtype=X.dtype)\n",
    "        for k in range(n):\n",
    "            for k2 in range(c):\n",
    "                Xr[k,k2]=np.rot90(X[k,k2],rotationAngle/90)\n",
    "            #Xr[k]=np.transpose(X[k],(0,2,1))\n",
    "\n",
    "        ff_traintestR=h5py.File(path2traintestR,'w')\n",
    "        ff_traintestR['X']=Xr\n",
    "        ff_traintestR['y']=y    \n",
    "        ff_traintestR['ids']=np.array(ids,'string')    \n",
    "        ff_traintestR.close()\n",
    "        print ('hdf5 saved!')\n",
    "    else:\n",
    "        print(path2traintestR +' rotated data exists!')\n",
    "        # load train-test data\n",
    "        ff_traintestR=h5py.File(path2traintestR,'r')\n",
    "        X=ff_traintestR['X']\n",
    "        print ('X shape', X.shape)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage1 train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# downsample/stack data into one array for train-validation\n",
    "data_type=\"stage1_train\"\n",
    "prefix=\"stage1_traintest\"\n",
    "\n",
    "h,w=256,330\n",
    "path2traintest=path2data+prefix+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "if not os.path.exists(path2traintest):\n",
    "    print ('wait ...')\n",
    "    X,y,ids=get_data(data_type)\n",
    "    ff_traintest=h5py.File(path2traintest,'w')\n",
    "    ff_traintest['X']=X\n",
    "    ff_traintest['y']=y    \n",
    "    ff_traintest['ids']=np.array(ids,'string')    \n",
    "    ff_traintest.close()\n",
    "    print ('hdf5 saved!')\n",
    "else:\n",
    "    print(path2traintest+ \" exists!\")\n",
    "    # load train-test data\n",
    "    ff_traintest=h5py.File(path2traintest,'r')\n",
    "    ids=ff_traintest['ids'].value\n",
    "    X=ff_traintest['X']\n",
    "    print ('X shape', X.shape)    \n",
    "    print('-'*50)\n",
    "    \n",
    "# we also want rotated data\n",
    "rotationAngles=[90,180,270]\n",
    "for rotAng in rotationAngles:    \n",
    "    rotateData(prefix,(h,w),rotAng)    \n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# downsample/stack data into one array for train-validation\n",
    "data_type=\"stage1_train\"\n",
    "prefix=\"stage1_traintest\"\n",
    "\n",
    "h,w=512,660\n",
    "path2traintest=path2data+prefix+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "if not os.path.exists(path2traintest):\n",
    "    print ('wait ...')\n",
    "    X,y,ids=get_data(data_type)\n",
    "    ff_traintest=h5py.File(path2traintest,'w')\n",
    "    ff_traintest['X']=X\n",
    "    ff_traintest['y']=y    \n",
    "    ff_traintest['ids']=np.array(ids,'string')    \n",
    "    ff_traintest.close()\n",
    "    print ('hdf5 saved!')\n",
    "else:\n",
    "    print(path2traintest+ \" exists!\")\n",
    "    # load train-test data\n",
    "    ff_traintest=h5py.File(path2traintest,'r')\n",
    "    ids=ff_traintest['ids'].value\n",
    "    X=ff_traintest['X']\n",
    "    print ('X shape', X.shape)    \n",
    "    print('-'*50)\n",
    "    \n",
    "# we also want rotated data\n",
    "rotationAngles=[180]\n",
    "for rotAng in rotationAngles:    \n",
    "    rotateData(prefix,(h,w),rotAng)    \n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage1 leaderboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# downsample/stack data into one array for train-validation\n",
    "data_type=\"stage1_leader\"\n",
    "prefix=\"stage1_leader\"\n",
    "\n",
    "h,w=256,330\n",
    "path2traintest=path2data+prefix+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "if not os.path.exists(path2traintest):\n",
    "    print ('wait ...')\n",
    "    X,y,ids=get_data(data_type)\n",
    "    ff_traintest=h5py.File(path2traintest,'w')\n",
    "    ff_traintest['X']=X\n",
    "    ff_traintest['y']=y    \n",
    "    ff_traintest['ids']=np.array(ids,'string')    \n",
    "    ff_traintest.close()\n",
    "    print ('hdf5 saved!')\n",
    "else:\n",
    "    print(path2traintest+ \" exists!\")\n",
    "    # load train-test data\n",
    "    ff_traintest=h5py.File(path2traintest,'r')\n",
    "    ids=ff_traintest['ids'].value\n",
    "    X=ff_traintest['X']\n",
    "    print ('X shape', X.shape)    \n",
    "    print('-'*50)    \n",
    "    \n",
    "# we also want rotated data\n",
    "rotationAngles=[90,180,270]\n",
    "for rotAng in rotationAngles:    \n",
    "    rotateData(prefix,(h,w),rotAng)    \n",
    "    print('-'*50)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# downsample/stack data into one array for train-validation\n",
    "data_type=\"stage1_leader\"\n",
    "prefix=\"stage1_leader\"\n",
    "\n",
    "h,w=512,660\n",
    "path2traintest=path2data+prefix+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "if not os.path.exists(path2traintest):\n",
    "    print ('wait ...')\n",
    "    X,y,ids=get_data(data_type)\n",
    "    ff_traintest=h5py.File(path2traintest,'w')\n",
    "    ff_traintest['X']=X\n",
    "    ff_traintest['y']=y    \n",
    "    ff_traintest['ids']=np.array(ids,'string')    \n",
    "    ff_traintest.close()\n",
    "    print ('hdf5 saved!')\n",
    "else:\n",
    "    print(path2traintest+ \" exists!\")\n",
    "    # load train-test data\n",
    "    ff_traintest=h5py.File(path2traintest,'r')\n",
    "    ids=ff_traintest['ids'].value\n",
    "    X=ff_traintest['X']\n",
    "    print ('X shape', X.shape)    \n",
    "    print('-'*50)    \n",
    "    \n",
    "# we also want rotated data\n",
    "rotationAngles=[180]\n",
    "for rotAng in rotationAngles:    \n",
    "    rotateData(prefix,(h,w),rotAng)    \n",
    "    print('-'*50)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage2 leaderboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# downsample/stack data into one array for train-validation\n",
    "data_type=\"stage2_leader\"\n",
    "prefix=\"stage2_leader\"\n",
    "\n",
    "h,w=256,330\n",
    "path2traintest=path2data+prefix+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "if not os.path.exists(path2traintest):\n",
    "    print ('wait ...')\n",
    "    X,y,ids=get_data(data_type)\n",
    "    ff_traintest=h5py.File(path2traintest,'w')\n",
    "    ff_traintest['X']=X\n",
    "    ff_traintest['y']=y    \n",
    "    ff_traintest['ids']=np.array(ids,'string')    \n",
    "    ff_traintest.close()\n",
    "    print ('hdf5 saved!')\n",
    "else:\n",
    "    print(path2traintest+ \" exists!\")\n",
    "    # load train-test data\n",
    "    ff_traintest=h5py.File(path2traintest,'r')\n",
    "    ids=ff_traintest['ids'].value\n",
    "    X=ff_traintest['X']\n",
    "    print ('X shape', X.shape)    \n",
    "    print('-'*50)        \n",
    "    \n",
    "# we also want rotated data\n",
    "rotationAngles=[90,180,270]\n",
    "for rotAng in rotationAngles:    \n",
    "    rotateData(prefix,(h,w),rotAng)    \n",
    "    print('-'*50)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# downsample/stack data into one array for train-validation\n",
    "data_type=\"stage2_leader\"\n",
    "prefix=\"stage2_leader\"\n",
    "\n",
    "h,w=512,660\n",
    "path2traintest=path2data+prefix+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "\n",
    "if not os.path.exists(path2traintest):\n",
    "    print ('wait ...')\n",
    "    X,y,ids=get_data(data_type)\n",
    "    ff_traintest=h5py.File(path2traintest,'w')\n",
    "    ff_traintest['X']=X\n",
    "    ff_traintest['y']=y    \n",
    "    ff_traintest['ids']=np.array(ids,'string')    \n",
    "    ff_traintest.close()\n",
    "    print ('hdf5 saved!')\n",
    "else:\n",
    "    print(path2traintest+ \" exists!\")\n",
    "    # load train-test data\n",
    "    ff_traintest=h5py.File(path2traintest,'r')\n",
    "    ids=ff_traintest['ids'].value\n",
    "    X=ff_traintest['X']\n",
    "    print ('X shape', X.shape)    \n",
    "    print('-'*50)        \n",
    "    \n",
    "# we also want rotated data\n",
    "rotationAngles=[180]\n",
    "for rotAng in rotationAngles:    \n",
    "    rotateData(prefix,(h,w),rotAng)    \n",
    "    print('-'*50)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
