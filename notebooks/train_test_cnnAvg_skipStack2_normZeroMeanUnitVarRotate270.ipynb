{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Homeland Security Competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu0,floatX=float32\" \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import h5py\n",
    "#import models\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import datetime\n",
    "\n",
    "# get package versions\n",
    "def get_version(*vars):\n",
    "    for var in vars:\n",
    "        module = __import__(var)    \n",
    "        print ('%s: %s' %(var,module.__version__))\n",
    "    \n",
    "# package version    \n",
    "get_version('keras','numpy','matplotlib','cv2','theano')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target image size\n",
    "h,w=330,256\n",
    "hh,ww=w,h\n",
    "\n",
    "# Parameter “pre_train” \n",
    "# if pre_train=False and  weights.hdf5 does not exist then we perform training from scratch\n",
    "# if pre_train=False and  weights.hdf5 exists then we load weights and perform prediction only \n",
    "# if pre_train=True  and  weights.hdf5 exists then we load weights and continue training\n",
    "# if pre_train=True  and weights.hdf5 does not exist then it raises an Error!\n",
    "pre_train=False\n",
    "\n",
    "# initial learning rate\n",
    "initLearningRate=4e-4\n",
    "\n",
    "# normalization \n",
    "#norm_type='minus1plus1'\n",
    "norm_type='zeroMeanUnitStd'\n",
    "\n",
    "# data folder\n",
    "path2data='../data/'\n",
    "\n",
    "# stage 1 training data\n",
    "path2stage1_train=path2data+'stage1_train.hdf5'\n",
    "\n",
    "# stage 1 test data\n",
    "path2stage1_test=path2data+'stage1_test.hdf5'\n",
    "path2stage2_test=path2data+'stage2_test.hdf5'\n",
    "\n",
    "# stage 1 sample submission\n",
    "path2stage1submission = path2data+'stage1_sample_submission.csv'\n",
    "path2stage2submission = path2data+'stage2_sample_submission.csv'\n",
    "\n",
    "# stage 1 labels\n",
    "path2external='/media/mra/win71/data/misc/kaggle'\n",
    "#path2_stage1_labels=path2external+\"/homeland/data/stage1_labels.csv\"\n",
    "path2_stage1_labels=path2data+\"stage1_labels.csv\"\n",
    "path2_stage1_solution=path2data+\"stage1_solution.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# zone orders in the csv file are like this\n",
    "zones=['zone1','zone10','zone11','zone12',\n",
    "       'zone13','zone14','zone15','zone16',\n",
    "       'zone17','zone2','zone3','zone4',\n",
    "       'zone5','zone6','zone7','zone8','zone9']\n",
    "\n",
    "body_zone_desc={\n",
    "        'zone1' :'Right Bicep',\n",
    "        'zone10':'Upper left Hip/thigh',\n",
    "        'zone11':'Lowe Right Thigh',\n",
    "        'zone12':'Lower left Thigh',\n",
    "        'zone13':'Right Calf',\n",
    "        'zone14':'Left Calf(below knee)',\n",
    "        'zone15':'Right Ankle Bone',\n",
    "        'zone16':'Left Ankle Bone',\n",
    "        'zone17':'Upper Back',    \n",
    "        'zone2':'Right Forearm',\n",
    "        'zone3':'Left Bicep',\n",
    "        'zone4':'Left Forearm',\n",
    "        'zone5':'Upper Chest',\n",
    "        'zone6':'Right Rib Cage and Abs',\n",
    "        'zone7':'Left Side Rib Cage and Abs',\n",
    "        'zone8':'Upper Right Hip/Tigh',\n",
    "        'zone9':'Groin (Sensetive area)'\n",
    "        } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def array_stats(X):\n",
    "    X=np.asarray(X)\n",
    "    print ('array shape: ',X.shape, X.dtype)\n",
    "    #print 'min: %.3f, max:%.3f, avg: %.3f, std:%.3f' %(np.min(X),np.max(X),np.mean(X),np.std(X))\n",
    "    print ('min: {}, max: {}, avg: {:.3}, std:{:.3}'.format( np.min(X),np.max(X),np.mean(X),np.std(X)))\n",
    "\n",
    "# gamma correction        \n",
    "def gammaAug(images,gamma):\n",
    "    # images shape: N*C*H*W\n",
    "    #print ('before gamma:', np.mean(image))\n",
    "    n1,c1,_,_=images.shape\n",
    "    for k1 in range(n1):\n",
    "        #for k2 in range(c1):\n",
    "        g = (2 * np.random.rand() - 1) * gamma + 1.\n",
    "        images[k1]=(images[k1]) ** g\n",
    "    #print ('after gamma:', np.mean(image))\n",
    "    return images\n",
    "    \n",
    "def preprocess(X,xnormType=None):\n",
    "    if xnormType=='minus1plus1':\n",
    "        X=X.astype('float32')\n",
    "        X/=np.max(X)\n",
    "        X-=0.5\n",
    "        X=X*2\n",
    "    elif xnormType=='zeroMeanUnitStd':\n",
    "        X=X.astype('float32')\n",
    "        X-=np.mean(X)\n",
    "        stdX=np.std(X)\n",
    "        if stdX>0.0:\n",
    "            X/=stdX\n",
    "    else:\n",
    "        print('no normalization!')\n",
    "    return X\n",
    "    \n",
    "# train test model\n",
    "def train_test_model(X_train,y_train,X_test,y_test,params_train):\n",
    "    foldnm=params_train['foldnm']  \n",
    "    pre_train=params_train['pre_train'] \n",
    "    batch_size=params_train['batch_size'] \n",
    "    augmentation=params_train['augmentation'] \n",
    "    gamma=params_train['gamma'] \n",
    "    print('batch_size: %s, Augmentation: %s' %(batch_size,augmentation))\n",
    "    \n",
    "    print 'fold %s training in progress ...' %foldnm\n",
    "    # load last weights\n",
    "    if pre_train== True:\n",
    "        if  os.path.exists(path2weights):\n",
    "            model.load_weights(path2weights)\n",
    "            print 'previous weights loaded!'\n",
    "        else:\n",
    "            raise IOError('weights does not exist!!!')\n",
    "    else:\n",
    "        if  os.path.exists(path2weights):\n",
    "            model.load_weights(path2weights)\n",
    "            print 'previous weights loaded!'\n",
    "            train_status='previous weights'\n",
    "            return train_status\n",
    "    \n",
    "    # path to csv file to save scores\n",
    "    path2scorescsv = weightfolder+'/scores.csv'\n",
    "    first_row = 'train,test'\n",
    "    with open(path2scorescsv, 'w+') as f:\n",
    "        f.write(first_row + '\\n')\n",
    "           \n",
    "    # Fit the model\n",
    "    start_time=time.time()\n",
    "    scores_test=[]\n",
    "    scores_train=[]\n",
    "    if params_train['loss']=='dice': \n",
    "        best_score = 0\n",
    "        previous_score = 0\n",
    "    else:\n",
    "        best_score = 1e6\n",
    "        previous_score = 1e6\n",
    "    patience = 0\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    #y_train = np_utils.to_categorical(y_train, nb_outputs)\n",
    "    #y_test = np_utils.to_categorical(y_test, nb_outputs)\n",
    "    \n",
    "    \n",
    "    for epoch in range(params_train['nbepoch']):\n",
    "    \n",
    "        print ('epoch: %s,  Current Learning Rate: %.1e' %(epoch,model.optimizer.lr.get_value()))\n",
    "        #seed = np.random.randint(0, 999999)\n",
    "    \n",
    "        #batches=0\n",
    "        #bs=X_train.shape[0]\n",
    "        #for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=bs):\n",
    "            #X_batch=preprocess_input(X_batch)\n",
    "            #hist = model.fit(X_batch, y_batch,verbose=0,batch_size=batch_size)\n",
    "            #batches += 1\n",
    "            #if batches >= len(X_train) / bs:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                #break\n",
    "            \n",
    "        #hist=model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                                 #samples_per_epoch=len(X_train), nb_epoch=1, verbose=0)\n",
    "    \n",
    "        if augmentation:\n",
    "            batches=0\n",
    "            bs=len(X_train)#batch_size*10\n",
    "            for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=bs,shuffle=False):\n",
    "                #X_batch=gammaAug(X_batch,gamma)\n",
    "                X_batch=preprocess(X_batch,norm_type)\n",
    "                hist=model.fit(X_batch[:,:,np.newaxis], y_batch, batch_size=batch_size,nb_epoch=1, verbose=0)\n",
    "                batches += 1\n",
    "                if batches >= len(X_train) / bs:\n",
    "                    # we need to break the loop by hand because\n",
    "                    # the generator loops indefinitely\n",
    "                    break\n",
    "        else:\n",
    "            hist=model.fit(preprocess(X_train,norm_type)[:,:,np.newaxis], y_train, batch_size=batch_size,nb_epoch=1, verbose=0)\n",
    "            \n",
    "        # evaluate on test and train data\n",
    "        score_test=model.evaluate(preprocess(X_test,norm_type)[:,:,np.newaxis],y_test,verbose=0,batch_size=batch_size)\n",
    "        score_train=np.mean(hist.history['loss'])\n",
    "       \n",
    "        print ('score_train: %s, score_test: %s' %(score_train,score_test))\n",
    "        scores_test=np.append(scores_test,score_test)\n",
    "        scores_train=np.append(scores_train,score_train)    \n",
    "\n",
    "        # check for improvement    \n",
    "        if (score_test<=best_score):\n",
    "            print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!! viva, improvement!!!!!!!!!!!!!!!!!!!!!!!!!!!\") \n",
    "            best_score = score_test\n",
    "            patience = 0\n",
    "            model.save_weights(path2weights)  \n",
    "            model.save(weightfolder+'/model.h5')\n",
    "            \n",
    "        # learning rate schedule\n",
    "        if score_test>previous_score:\n",
    "            #print \"Incrementing Patience.\"\n",
    "            patience += 1\n",
    "\n",
    "        # learning rate schedule                \n",
    "        if patience == params_train['max_patience']:\n",
    "            params_train['learning_rate'] = params_train['learning_rate']/2\n",
    "            print (\"Upating Current Learning Rate to: \", params_train['learning_rate'])\n",
    "            model.optimizer.lr.set_value(params_train['learning_rate'])\n",
    "            print (\"Loading the best weights again. best_score: \",best_score)\n",
    "            model.load_weights(path2weights)\n",
    "            patience = 0\n",
    "        \n",
    "        # save current test score\n",
    "        previous_score = score_test    \n",
    "        \n",
    "        # store scores into csv file\n",
    "        with open(path2scorescsv, 'a') as f:\n",
    "            string = str([score_train,score_test])\n",
    "            f.write(string + '\\n')\n",
    "\n",
    "        # train test progress plots\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.plot(scores_test)\n",
    "        plt.plot(scores_train)\n",
    "        plt.title('train-validation progress',fontsize=20)\n",
    "        plt.legend(('test','train'), loc = 'top right',fontsize=20)\n",
    "        plt.xlabel('epochs',fontsize=20)\n",
    "        plt.ylabel('loss',fontsize=20)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.savefig(weightfolder+'/train_val_progress.png')\n",
    "            \n",
    "    \n",
    "    print ('model was trained!')\n",
    "    elapsed_time=(time.time()-start_time)/60\n",
    "    print ('elapsed time: %d  mins' %elapsed_time)      \n",
    "\n",
    "    # train test progress plots\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(scores_test)\n",
    "    plt.plot(scores_train)\n",
    "    plt.title('train-validation progress',fontsize=20)\n",
    "    plt.legend(('test','train'), loc = 'top right',fontsize=20)\n",
    "    plt.xlabel('epochs',fontsize=20)\n",
    "    plt.ylabel('loss',fontsize=20)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.savefig(weightfolder+'/train_val_progress.png')\n",
    "    \n",
    "    print 'training completed!'\n",
    "    train_status='completed!'\n",
    "    return train_status \n",
    "\n",
    "# display a sample subject\n",
    "def dispSampleSubject(X,y_zone=None): \n",
    "    # X shape: N*C*H*W\n",
    "    sbj_num=np.random.randint(len(X))\n",
    "    print ('subject: %s' %sbj_num)\n",
    "    #array_stats(X[sbj_num])\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for k in range(16):\n",
    "        plt.subplot(4,4,k+1)\n",
    "        plt.imshow(X[sbj_num,k],cmap='gray')\n",
    "    if y_zone is not None:\n",
    "        # zones with objects\n",
    "        nz_label=np.nonzero(y_zone[sbj_num,:])[0]\n",
    "        for nz_l in nz_label:\n",
    "            print ('%s: %s' %(zones[nz_l],body_zone_desc[zones[nz_l]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(data_type='stage1_train'):\n",
    "    # number of zones\n",
    "    nb_zones=17\n",
    "\n",
    "    if data_type=='stage1_train':\n",
    "        ff=h5py.File(path2stage1_train,'r')\n",
    "        # get subject ids\n",
    "        keys=ff.keys()\n",
    "        ids=np.array(keys[:-1])\n",
    "        y=ff['labels'].value\n",
    "    elif data_type=='stage1_leader':\n",
    "        ff=h5py.File(path2stage1_test,'r') \n",
    "        keys=ff.keys()\n",
    "        ids=np.array(keys)\n",
    "        y=np.zeros((nb_zones,len(ids)),'uint8')\n",
    "    elif data_type=='stage2_leader':\n",
    "        ff=h5py.File(path2stage2_test,'r') \n",
    "        keys=ff.keys()\n",
    "        ids=np.array(keys)\n",
    "        y=np.zeros((nb_zones,len(ids)),'uint8')\n",
    "        \n",
    "        \n",
    "    X=[]\n",
    "    for k,id1 in enumerate(ids):\n",
    "        print k,id1\n",
    "        X0= ff[id1].value\n",
    "        X0=cv2.resize(X0,(w,h),interpolation = cv2.INTER_CUBIC)\n",
    "        X.append(X0)\n",
    "    X=np.stack(X)\n",
    "    X=np.transpose(X,(0,3,1,2))\n",
    "    return X,y.astype('uint8'),ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this was to rotate images\n",
    "def rotateData(data_type,(h,w)):\n",
    "    path2traintest=path2data+data_type+'_'+str(w)+'by'+str(h)+'.hdf5'\n",
    "    \n",
    "    path2traintestR=path2data+data_type+'270'+'_'+str(h)+'by'+str(w)+'.hdf5'\n",
    "\n",
    "    # load train-test data\n",
    "    if os.path.exists(path2traintest):\n",
    "        print ('loading: %s' %path2traintest)\n",
    "        ff_traintest=h5py.File(path2traintest,'r')\n",
    "        X=ff_traintest['X']\n",
    "        try:\n",
    "            y=ff_traintest['y'].value\n",
    "        except:\n",
    "            y=np.zeros(len(X))\n",
    "        ids=ff_traintest['ids'].value\n",
    "    else:\n",
    "        print ('source data does not exist!')\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(path2traintestR):\n",
    "        print('wait ...')\n",
    "        ## rotate\n",
    "        n,c,w,h=X.shape\n",
    "        Xr=np.zeros((n,c,h,w),dtype=X.dtype)\n",
    "        for k in range(n):\n",
    "            Xr[k]=np.rot90(X[k],k=3,axes=(1,2))\n",
    "        \n",
    "        print ('storing: %s' %path2traintestR)\n",
    "        ff_traintestR=h5py.File(path2traintestR,'w')\n",
    "        ff_traintestR['X']=Xr\n",
    "        ff_traintestR['y']=y    \n",
    "        ff_traintestR['ids']=np.array(ids,'string')    \n",
    "        ff_traintestR.close()\n",
    "        print 'hdf5 saved!'\n",
    "    else:\n",
    "        print('rotated data exists!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datatype='stage1_traintest'\n",
    "#rotateData(datatype,(h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path to train-test.hdf5, first checking if it exists\n",
    "path2traintest=path2data+'stage1_traintest270_'+str(hh)+'by'+str(ww)+'.hdf5'\n",
    "if not os.path.exists(path2traintest):\n",
    "    raise IOError(path2traintest+' data does not exist!')\n",
    "    #print ('wait ...')\n",
    "    #X,y,ids=get_data('stage1_train')\n",
    "    #ff_traintest=h5py.File(path2traintest,'w')\n",
    "    #ff_traintest['X']=X\n",
    "    #ff_traintest['y']=y    \n",
    "    #ff_traintest['ids']=np.array(ids,'string')    \n",
    "    #ff_traintest.close()\n",
    "    #print 'hdf5 saved!'\n",
    "\n",
    "# load train-test data\n",
    "ff_traintest=h5py.File(path2traintest,'r')\n",
    "ids=ff_traintest['ids'].value\n",
    "\n",
    "\n",
    "# read labels\n",
    "stage1_train=pd.read_csv(path2_stage1_labels)\n",
    "\n",
    "# labels\n",
    "nb_zones=17\n",
    "Probability=stage1_train.Probability\n",
    "y_zone=np.array(Probability)\n",
    "y_zone=np.reshape(y_zone,(len(y_zone)/nb_zones,nb_zones))\n",
    "print 'labels shape:', y_zone.shape\n",
    "\n",
    "\n",
    "# k-fold cross-validation data split\n",
    "X=ff_traintest['X']\n",
    "print ('X, y_zone:', X.shape,y_zone.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispSampleSubject(X,y_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### defining AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D,Dropout\n",
    "from keras.layers import Activation,Reshape,Permute,Flatten,Dense\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import RMSprop, Adam,Nadam\n",
    "from keras.layers import GlobalAveragePooling1D,AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.advanced_activations import ELU,PReLU,LeakyReLU\n",
    "\n",
    "def conv_block(x_input, num_filters,pool=True,norm=False,drop_rate=0.0,border_mode='same',stride=1):\n",
    "    x1 = Convolution2D(num_filters,3,3,subsample=(stride,stride),border_mode=border_mode,W_regularizer=l2(1e-4))(x_input)\n",
    "    if norm:\n",
    "        x1 = BatchNormalization(mode=2,axis=1)(x1)\n",
    "        \n",
    "    if drop_rate > 0.0:\n",
    "        x1 = GaussianDropout(drop_rate)(x1)\n",
    "\n",
    "    x1 = LeakyReLU(.1)(x1)\n",
    "    if pool:\n",
    "        x1 = MaxPooling2D()(x1)\n",
    "    x_out = x1\n",
    "    return x_out\n",
    "\n",
    "\n",
    "def modelCnnAvg(params):\n",
    "    # input format: batch_size*timesteps*z*h*w    \n",
    "    h=params['h']\n",
    "    w=params['w']\n",
    "    z=params['z']\n",
    "    timesteps=params['timesteps']    \n",
    "    lr=params['learning_rate']\n",
    "    nb_output=params['nb_outputs']\n",
    "    loss=params['loss']\n",
    "    C=params['init_filters']\n",
    "    border_mode=params['border_mode']\n",
    "    base_display=params['base_display']\n",
    "    stride=params['stride']\n",
    "    dropOutRate=params['dropOutRate']\n",
    "    optimizer=params['optimizer']\n",
    "    kS=params['kernelSize']\n",
    "    kI=params['kernelInit']\n",
    "    bnEnable=params['batchNormEnable']\n",
    "    iF=params['init_filters']\n",
    "    \n",
    "  \n",
    "    xin = Input((z,h, w))\n",
    "    x1 = conv_block(xin,iF,norm=bnEnable,drop_rate=0,stride=stride) \n",
    "    x1_ident = AveragePooling2D(pool_size=(2*stride, 2*stride))(xin)\n",
    "    x1_merged = merge([x1, x1_ident],mode='concat', concat_axis=1)\n",
    "\n",
    "    x2_1 = conv_block(x1_merged,3*iF,norm=bnEnable,drop_rate=0) \n",
    "    x2_1 = conv_block(x2_1,3*iF,norm=bnEnable,pool=False,drop_rate=0) \n",
    "    x2_ident = AveragePooling2D()(x1_ident)\n",
    "    x2_merged = merge([x2_1,x2_ident],mode='concat', concat_axis=1)\n",
    "\n",
    "    #by branching we reduce the #params\n",
    "    x3_1 = conv_block(x2_merged,8*iF,norm=bnEnable,drop_rate=0) \n",
    "    x3_1 = conv_block(x3_1,8*iF,norm=bnEnable,pool=False,drop_rate=0) \n",
    "    x3_ident = AveragePooling2D()(x2_ident)\n",
    "    x3_merged = merge([x3_1,x3_ident],mode='concat', concat_axis=1)\n",
    "\n",
    "    x4_1 = conv_block(x3_merged,9*iF,norm=bnEnable,drop_rate=0) \n",
    "    x4_1 = conv_block(x4_1,9*iF,norm=bnEnable,pool=False,drop_rate=0) \n",
    "    x4_ident = AveragePooling2D()(x3_ident)\n",
    "    x4_merged = merge([x4_1,x4_ident],mode='concat', concat_axis=1)    \n",
    "\n",
    "    x5_1 = conv_block(x4_merged,9*iF,norm=bnEnable,pool=False,drop_rate=0) \n",
    "    x5_1 = conv_block(x5_1,9*iF,norm=bnEnable,pool=False,drop_rate=0) \n",
    "    \n",
    "    # last layer of encoding    \n",
    "    x=Flatten() (x5_1)\n",
    "    \n",
    "    model_encoder = Model(input=xin, output=x)\n",
    "    if base_display:\n",
    "        model_encoder.summary()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(model_encoder,input_shape=(timesteps,z,h,w)))    \n",
    "\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(dropOutRate))\n",
    "    model.add(Dense(nb_output, activation='sigmoid',W_regularizer=l2(1e-4)))\n",
    "    \n",
    "    if optimizer=='RMSprop':\n",
    "        optimizer = RMSprop(lr)\n",
    "    elif optimizer=='Adam':       \n",
    "        optimizer = Adam(lr)\n",
    "    elif optimizer=='Nadam':       \n",
    "        optimizer = Nadam(lr,clipvalue=1.0)        \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "# random data generator\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.10,\n",
    "        height_shift_range=0.10,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.15,\n",
    "        channel_shift_range=0.0,\n",
    "        fill_mode='constant',\n",
    "        cval=0.0,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        dim_ordering='th') \n",
    "\n",
    "\n",
    "\n",
    "# training params\n",
    "params_train={\n",
    "    'h': h, # for models trained on rotated data, swap h and w\n",
    "    'w': w,\n",
    "    'z': 1,\n",
    "    'timesteps':X.shape[1],\n",
    "    'c':1,           \n",
    "    'learning_rate': initLearningRate,\n",
    "    #'optimizer': 'Adam',\n",
    "    'optimizer': 'Nadam',\n",
    "    #'loss': 'categorical_crossentropy',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    #'loss': 'mean_squared_error',\n",
    "    'nbepoch': 1000,\n",
    "    'nb_outputs': 17,\n",
    "    'init_filters': 24,    \n",
    "    'max_patience': 30,\n",
    "    'stride': 2,\n",
    "    'fully_connected': False,\n",
    "    'output_activation': 'sigmoid',\n",
    "    'border_mode':'valid',\n",
    "    'base_display': True,\n",
    "    'dropOutRate': 0.5,\n",
    "    'pre_train': False,\n",
    "    'foldnm':1,\n",
    "    'kernelSize':3,\n",
    "    'kernelInit':'glorot_uniform',\n",
    "    'batchNormEnable': True,\n",
    "    'batch_size': 8,\n",
    "    'gamma': 0,\n",
    "    'augmentation': True,\n",
    "}    \n",
    "\n",
    "\n",
    "model=modelCnnAvg(params_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-fold train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_folds=1\n",
    "#skf = StratifiedKFold(n_splits=5,random_state=123,shuffle=True)\n",
    "#skf = KFold(n_splits=n_folds,random_state=234)\n",
    "skf = ShuffleSplit(n_splits=n_folds, test_size=0.1, random_state=321)\n",
    "\n",
    "# loop over folds\n",
    "foldnm=0\n",
    "scores_nfolds=[]\n",
    "\n",
    "print ('wait ...')\n",
    "for train_ind, test_ind in skf.split(X,y_zone):\n",
    "    foldnm+=1    \n",
    "\n",
    "    train_ind=list(np.sort(train_ind))\n",
    "    test_ind=list(np.sort(test_ind))\n",
    "    #print train_ind, train_ind\n",
    "    #train_ind=train_ind[:10]\n",
    "    #test_ind=test_ind[:10]\n",
    "    \n",
    "    X_train,y_train=X[train_ind],y_zone[train_ind]\n",
    "    X_test,y_test=X[test_ind],y_zone[test_ind]\n",
    "    \n",
    "    array_stats(X_train)\n",
    "    array_stats(y_train)\n",
    "    array_stats(X_test)\n",
    "    array_stats(y_test)\n",
    "    print ('-'*30)\n",
    "\n",
    "   # training params\n",
    "    params_train={\n",
    "        'h': h,\n",
    "        'w': w,\n",
    "        'z': 1,\n",
    "        'timesteps':X.shape[1],\n",
    "        'c':1,           \n",
    "        'learning_rate': initLearningRate,\n",
    "        #'optimizer': 'Adam',\n",
    "        'optimizer': 'Nadam',\n",
    "        #'loss': 'categorical_crossentropy',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        #'loss': 'mean_squared_error',\n",
    "        'nbepoch': 1000,\n",
    "        'nb_outputs': 17,\n",
    "        'init_filters': 24,    \n",
    "        'max_patience': 30,\n",
    "        'stride': 2,\n",
    "        'fully_connected': False,\n",
    "        'output_activation': 'sigmoid',\n",
    "        'border_mode':'same',\n",
    "        'base_display': True,\n",
    "        'dropOutRate': 0.0,\n",
    "        'pre_train': pre_train,\n",
    "        'foldnm':foldnm,\n",
    "        'kernelSize':3,\n",
    "        'kernelInit':'glorot_uniform',\n",
    "        'batchNormEnable': True,\n",
    "        'batch_size': 8,\n",
    "        'augmentation': True,\n",
    "        'gamma': 0, # gamma correction\n",
    "    }    \n",
    "    \n",
    "    model=modelCnnAvg(params_train)\n",
    "    model.summary()\n",
    "     \n",
    "    \n",
    "    # exeriment name to record weights and scores\n",
    "    netinfo='_cnnSkipStack2GlobalAvg_normZeroMeanUnitVarRotate270'\n",
    "    experiment='fold'+str(foldnm)+netinfo+'_hw_'+str(h)+'by'+str(w)+'_initfilts_'+str(params_train['init_filters'])\n",
    "    print ('experiment:', experiment)\n",
    "\n",
    "    # checkpoint\n",
    "    weightfolder='./output/weights/'+experiment\n",
    "    if  not os.path.exists(weightfolder):\n",
    "        os.makedirs(weightfolder)\n",
    "    print ('weights folder created')    \n",
    "    \n",
    "    # path to weights\n",
    "    path2weights=weightfolder+\"/weights.hdf5\"\n",
    "    path2model=weightfolder+\"/model.hdf5\"    \n",
    "    \n",
    "    # train test on fold #\n",
    "    train_test_model(X_train,y_train,X_test,y_test,params_train)\n",
    "    \n",
    "    # loading best weights from training session\n",
    "    if  os.path.exists(path2weights):\n",
    "        model.load_weights(path2weights)\n",
    "        print 'weights loaded!'\n",
    "    else:\n",
    "        raise IOError('weights does not exist!!!')\n",
    "    #y_test = np_utils.to_categorical(y_test, nb_outputs)\n",
    "    score_test=model.evaluate(preprocess(X_test,norm_type)[:,:,np.newaxis],y_test,verbose=0,batch_size=8)\n",
    "    print ('score_test: %.5f' %(score_test))    \n",
    "    print ('-' *30)\n",
    "    # store scores for all folds\n",
    "    scores_nfolds.append(score_test)\n",
    "\n",
    "print ('average score for %s folds is %s' %(n_folds,np.mean(scores_nfolds)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training progress plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotScores(weightfolder):\n",
    "    # path to csv file to save scores\n",
    "    path2scorescsv = weightfolder+'/scores.csv'\n",
    "    scores=[]\n",
    "    with open(path2scorescsv, 'r') as f:\n",
    "        scores.append(f.read())\n",
    "\n",
    "    scores=scores[0].split('\\n')\n",
    "    scores=scores[1:] # train test\n",
    "\n",
    "    scoresTrain=[]\n",
    "    scoresTest=[]\n",
    "    for row in scores:\n",
    "\n",
    "        tmp=row.split(',')\n",
    "        if len(tmp)>1:\n",
    "            tmpTrain=float(tmp[0][1:])\n",
    "            tmpTest=float(tmp[1][1:-1])\n",
    "            scoresTrain.append(tmpTrain)\n",
    "            scoresTest.append(tmpTest)\n",
    "    plt.figure(figsize=(10,5))            \n",
    "    plt.plot(scoresTrain);\n",
    "    plt.plot(scoresTest);\n",
    "    plt.legend(['Train','Test'])\n",
    "    plt.grid()\n",
    "plotScores(weightfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(path2weights)\n",
    "yPred=model.predict(preprocess(X_test,norm_type)[:,:,np.newaxis])\n",
    "print (yPred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    sumY=[]\n",
    "    n1,n2=y_true.shape\n",
    "    for i1 in range(n1):\n",
    "        for i2 in range(n2):\n",
    "            yi=y_true[i1,i2]\n",
    "            yih=y_pred[i1,i2]\n",
    "            # clip outputs\n",
    "            yi=max(min(yi,1-10**(-15)),10**(-15))\n",
    "            yih=max(min(yih,1-10**(-15)),10**(-15))\n",
    "            \n",
    "            # calculate log loss\n",
    "            p1=yi*np.log(yih)+(1-yi)*np.log(1-yih)\n",
    "            sumY.append(p1)\n",
    "    return -np.mean(sumY)\n",
    "\n",
    "print 'logloss:',logloss(y_test,yPred)\n",
    "#print 'logloss:',logloss(y_train,yPredTrain)\n",
    "print ('-'*30)\n",
    "\n",
    "print yPred.shape\n",
    "sumY=[]\n",
    "errorInds=[]\n",
    "for i1 in range(len(y_test)):\n",
    "    for i2 in range(17):\n",
    "        yi=y_test[i1,i2]\n",
    "        yi=max(min(yi,1-10**(-15)),10**(-15))\n",
    "        yih=yPred[i1,i2]\n",
    "        #if yih<10**(-6):\n",
    "        #    yih=0.0\n",
    "        #if yih>0.999:\n",
    "            #yih=1.0\n",
    "            \n",
    "        yih=max(min(yih,1-10**(-15)),10**(-15))\n",
    "        if (yi>.5) != (yih>.5):\n",
    "            print i1,i2,yi,yih    \n",
    "            errorInds.append(i1)\n",
    "        else:\n",
    "            p1=yi*np.log(yih)+(1-yi)*np.log(1-yih)\n",
    "            sumY.append(p1)\n",
    "print -np.mean(sumY)\n",
    "print len(sumY)\n",
    "print ('error indices in test:',errorInds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 1 leaderboard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rotate data\n",
    "#data_type='stage1_leader'\n",
    "#rotateData(data_type,(h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_type='stage1_leader270'\n",
    "\n",
    "# load leaderborad data\n",
    "path2_stage1leader=path2data+data_type+'_'+str(hh)+'by'+str(ww)+'.hdf5'\n",
    "if not os.path.exists(path2_stage1leader):\n",
    "    raise IOerro(path2_stage1leader +' does not exist!')\n",
    "    #print ('wait ...')\n",
    "    #X,y,ids=get_data(data_type)\n",
    "    #ff_stage1leader=h5py.File(path2_stage1leader,'w')\n",
    "    #ff_stage1leader['X']=X\n",
    "    #ff_stage1leader['y']=y    \n",
    "    #ff_stage1leader['ids']=np.array(ids,'string')    \n",
    "    #ff_stage1leader.close()\n",
    "    #print 'hdf5 saved!'\n",
    "\n",
    "# load train-test data\n",
    "ff_stage1leader=h5py.File(path2_stage1leader,'r')\n",
    "ids_stage1leader=ff_stage1leader['ids'].value\n",
    "\n",
    "X_stage1leader=ff_stage1leader['X'].value\n",
    "#X_stage1leader=preprocess(X_stage1leader,norm_type)\n",
    "array_stats(X_stage1leader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispSampleSubject(X_stage1leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## evaluate stage 1\n",
    "yPred1=model.predict(preprocess(X_stage1leader,norm_type)[:,:,np.newaxis])\n",
    "print (yPred1.shape)\n",
    "\n",
    "# read labels\n",
    "stage1_test=pd.read_csv(path2_stage1_solution)\n",
    "Probability=stage1_test.Probability\n",
    "y_zone1=np.array(Probability)\n",
    "y_stage1=np.reshape(y_zone1,(len(y_zone1)/nb_zones,nb_zones))\n",
    "print 'labels shape:', y_zone1.shape\n",
    "print 'logloss:',logloss(y_stage1,yPred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stage 2 leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rotate data\n",
    "#data_type='stage2_leader'\n",
    "#rotateData(data_type,(h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_type='stage2_leader270'\n",
    "\n",
    "# load leaderborad data\n",
    "path2_stage2leader=path2data+data_type+'_'+str(hh)+'by'+str(ww)+'.hdf5'\n",
    "if not os.path.exists(path2_stage2leader):\n",
    "    raise IOerro(path2_stage2leader +' does not exist!')\n",
    "    #print ('wait ...')\n",
    "    #X,y,ids=get_data(data_type)\n",
    "    #ff_stage2leader=h5py.File(path2_stage1leader,'w')\n",
    "    #ff_stage2leader['X']=X\n",
    "    #ff_stage1leader['y']=y    \n",
    "    #ff_stage2leader['ids']=np.array(ids,'string')    \n",
    "    #ff_stage2leader.close()\n",
    "    #print 'hdf5 saved!'\n",
    "\n",
    "# load train-test data\n",
    "ff_stage2leader=h5py.File(path2_stage2leader,'r')\n",
    "ids_stage2leader=ff_stage2leader['ids'].value\n",
    "\n",
    "X_stage2leader=ff_stage2leader['X'].value\n",
    "#X_stage1leader=preprocess(X_stage1leader,norm_type)\n",
    "array_stats(X_stage2leader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispSampleSubject(X_stage2leader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction for nfolds\n",
    "y_pred=[]\n",
    "for foldnm in range(1,n_folds+1):\n",
    "    \n",
    "    # load weights\n",
    "    experiment='fold'+str(foldnm)+netinfo+'_hw_'+str(h)+'by'+str(w)+'_initfilts_'+str(params_train['init_filters'])\n",
    "    print ('experiment:', experiment)\n",
    "    weightfolder='./output/weights/'+experiment\n",
    "    # path to weights\n",
    "    path2weights=weightfolder+\"/weights.hdf5\"\n",
    "    if  os.path.exists(path2weights):\n",
    "        model.load_weights(path2weights)\n",
    "        print ('weights loaded!')\n",
    "    else:\n",
    "        raise IOError ('weights does not exist!')\n",
    "\n",
    "    # prediction\n",
    "    y_pred_perfold=model.predict(preprocess(X_stage2leader,norm_type)[:,:,np.newaxis])\n",
    "    print y_pred_perfold.shape    \n",
    "    y_pred.append(y_pred_perfold)        \n",
    "        \n",
    "# reshape \n",
    "y_pred1=np.array(y_pred)\n",
    "y_pred2=np.mean(y_pred1,axis=0)\n",
    "\n",
    "r1,c1=y_pred2.shape\n",
    "print y_pred2[0:5]\n",
    "print (r1,c1)\n",
    "pred=np.reshape(y_pred2,(r1*c1,1))\n",
    "print (pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission  = pd.read_csv(path2stage2submission)\n",
    "pid = submission ['Id'].values\n",
    "\n",
    "# make submission\n",
    "now = datetime.datetime.now()\n",
    "info=experiment\n",
    "suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "path2submission = os.path.join('./output/submissions', 'submission_' + suffix + '.csv')\n",
    "\n",
    "#submission = pd.DataFrame(pred, columns=['Probability'])\n",
    "submission['Id'] = pid\n",
    "submission['Probability'] = pred\n",
    "submission.to_csv(path2submission, index=False)\n",
    "submission.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
